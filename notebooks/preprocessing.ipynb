{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6998f350",
   "metadata": {},
   "source": [
    "This notebook is for preprocessing the data and getting it ready to be clustered. This is done by exploring the data, getting rid of outliers, \n",
    "and converting data to binary to only check for presence of a feature. I chose to use a variance threshhold for some feature reduction by only \n",
    "getting rid of features where all values were identical. If a feature is just the exact same (present or not present) for every sample of data, it \n",
    "contriubutes nothing but noise. I chose not to use Principal Componenet Analysis (PCA) because it assumes the data is continuouse, not binary, causing \n",
    "potential issues due to assumptions. Multiple Correspondence Analysis (MCA) would also work for dimensionality reduction, but would likely cause a loss \n",
    "in information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1e094cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54c2bdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k_/qdqmhtsx7mz24yl5hmlfnjz80000gn/T/ipykernel_22593/1490154515.py:1: DtypeWarning: Columns (0,3,4,6,7,8,12,13,15,16,19,22,23,25,26,27,28,29,30,31,32,34,35,36,37,38,39,40,41,42,43,44,46,47,48,49,50,51,52,53,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,82,84,85,86,87,88,89,90,91,92,94,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,124,125,126,127,128,129,130,131,132,133,134,135,136,138,139,140,144,145,146,147,148,149,150,151,152,153,154,155,156,159,165,168,169,170,173,174,176,177,178,179,180,181,182,183,184,185,186,187,188,189,191,192,193,194,197,199,200,201,221,222,223,224,225,228,229,230,231,232,233,234,235,236,237,242,243,244,245,246,247,248,249,250,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,307,310,312,313,314,315,316,319,320,321,322,323,324,325,326,327,328,333,334,335,336,337,338,339,340,341,342,343,344,345,348,350,351,357,358,359,360,362,363,364,366,367,368,369,370,373,375,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,411,412,414,415,416,417,418,419,421,422,423,424,425,426,427,428,429,430,431,432,433,435,436,437,438,439,440,441,442,443,448,449,450,451,452,453,454,455,456,457,458,460,461,465,466,467,468,469,471,472,473,474,475,476,477,478) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"../data/parquets/raw_parquet.csv\")\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/parquets/raw_parquet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4c04b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this to see value counts of each column. The builtin value_counts function in the pandas libary is hard to read, so with val_count, it makes it a bit easier\n",
    "\n",
    "def val_count(data):\n",
    "    counter = 0\n",
    "    for i in data.columns:\n",
    "        print(counter)\n",
    "        print(data[i].value_counts())\n",
    "        counter += 1\n",
    "        print(\"==========================\"*5)\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfb7c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_count(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92abed9d",
   "metadata": {},
   "source": [
    "Some features may be unnecessary in the in analysis, especially features that pertain more to the use of the app and its features like line and circle colors for example. It is also a very necessary step to try and reduce the cardinality of the data when possible so as to reduce the noise later on and improve processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9b6c949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns Dropped:\n",
      "\tproperties_viewed_timestamp\n",
      "\tproperties_modified_timestamp\n",
      "\tproperties_symbology_circleColor\n",
      "\tproperties_id\n",
      "\tproperties_symbology_lineColor\n",
      "\tproperties_symbology_lineWidth\n",
      "\tproperties_symbology_lineDasharray\n",
      "\tproperties_sed_strat_section_strat_section_id\n",
      "\tproperties_strat_section_id\n",
      "\tproperties_symbology_fillColor\n",
      "\tproperties_orientation_id\n",
      "\tproperties_custom_fields_osm_id\n",
      "\tproperties_orientation_modified_timestamp\n",
      "\tproperties_custom_fields_id\n",
      "\tproperties_orientation_unix_timestamp\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = []\n",
    "for col in df.columns:\n",
    "    if re.search(\"^properties_symbology\", col):\n",
    "        columns_to_drop.append(col)\n",
    "    if re.search(\"_id$\", col):\n",
    "        columns_to_drop.append(col)\n",
    "    if re.search(\"_timestamp$\", col):\n",
    "        columns_to_drop.append(col)\n",
    "\n",
    "df = df.drop(columns= columns_to_drop)\n",
    "\n",
    "print(\"Columns Dropped:\")\n",
    "for col in columns_to_drop:\n",
    "    print(f\"\\t{col}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44a7693c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1359415, 465)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting rid of duplicates\n",
    "\n",
    "df.drop_duplicates(inplace = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e332d4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_simplification(df):\n",
    "    \"\"\"converts a pandas dataframe into binary based off of presence in data\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): a pandas dataframe of the data that needs to be converted to binary\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: a new dataframe that has now been converted to binary\n",
    "    \"\"\"\n",
    "    df_new = df.copy()\n",
    "    binary_col_data = {}\n",
    "    columns_to_drop = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # print(f\"Converting {col} to binary\")\n",
    "        binary_col_data[col] = df_new[col].replace('', np.nan).notna().astype(int)\n",
    "        columns_to_drop.append(col)\n",
    "            \n",
    "    df_new = df_new.drop(columns=columns_to_drop)\n",
    "    \n",
    "    # Add all new binary columns in one go using pd.concat\n",
    "    if binary_col_data:\n",
    "        df_new = pd.concat([df_new, pd.DataFrame(binary_col_data, index=df_new.index)], axis=1)\n",
    "        \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7b390ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = binary_simplification(df)\n",
    "# val_count(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73c26589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1359415, 442)\n"
     ]
    }
   ],
   "source": [
    "# uses variance threshold for feature reduction, removes features where all values are identical\n",
    "\n",
    "selector = VarianceThreshold(threshold=0) #removes all features with low variance in 100% of samples  (you do (1 - percentage of same values) * percentage of vaues that are the same)\n",
    "selector.fit_transform(df)\n",
    "\n",
    "cols_idxs = selector.get_support(indices=True)\n",
    "df = df.iloc[:,cols_idxs]\n",
    "\n",
    "print(df.shape)\n",
    "# val_count(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d0dfa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/parquets/processed_parquet.csv\", index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
